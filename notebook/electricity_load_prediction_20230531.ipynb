{"cells":[{"cell_type":"markdown","metadata":{"id":"PnpO3d1ufQJH"},"source":["* 아래 함수는 `weather_localcenter_metadata`와 `weather_5minute_ASOS_2020-2023.parquet`, `holiday.csv`를 사용하여 5분 단위 기상대표값을 도출합니다.\n","* 본 모델링에서는 기상인자로 최고기온, 최저기온, 평균일사량 데이터를 사용하고자 합니다. 일사량의 결측치 비율을 최소화 하기 위해 울산시를 제외한 특별시 및 광역시 데이터를 사용하였으며 (울산시 일사량 데이터의 대부분이 결측치였음), 강원도의 기상 인자도 반영하기 위해 도내에서 가장 인구가 많은 도시인 원주를 포함하였습니다.\n","* 보간법을 적용하기 전의 기온과 일사량의 결측치 비율은 전체 대비 0.2%, 0.4% 수준이며, 선형보간법을 통해 결측치를 보완하고자 했습니다.\n","* 단순히 휴일여부가 아니라 어떤 휴일에 해당하는지를 더미변수로 추가해보았습니다. (휴일이 아닌 날, 신정, 설날, 삼일절, 선거일, 어린이날, 석가탄신일, 현충일, 광복절, 추석, 개천절, 기독탄신일, 한글날, 대체공휴일 등 총 14가지로 분류)"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1660,"status":"ok","timestamp":1685482678223,"user":{"displayName":"누노누노","userId":"02523908322137540802"},"user_tz":-540},"id":"n1JP0_yDejIO"},"outputs":[],"source":["import random\n","import numpy as np\n","import torch\n","\n","seed = 222\n","\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":565,"status":"ok","timestamp":1685482719834,"user":{"displayName":"누노누노","userId":"02523908322137540802"},"user_tz":-540},"id":"pgEc7cGI9j35"},"outputs":[],"source":["def weather_data_5minute(weather_metadata_path, weather_5minute_ASOS_path, holiday_path):\n","\n","  import pandas as pd\n","\n","  # 기상관측 MetaData 불러오기\n","  weather_metadata = pd.read_csv(weather_metadata_path)\n","  STN_dict = {STN_name : STN_num for STN_num, STN_name in zip(weather_metadata['지점'], weather_metadata['지점명'])}\n","  target_STN_list = [STN_dict[STN_name] for STN_name in ['서울', '부산', '인천', '대구', '대전', '광주', '원주']]\n","\n","  # 서울 포함 7개 도시의 기온 및 일사량 데이터 불러오기\n","  weather_5minute = pd.read_parquet(weather_5minute_ASOS_path).astype({'지점' : 'int'})\n","  weather_5minute = weather_5minute[weather_5minute['지점'].isin(target_STN_list)]\n","  weather_5minute = weather_5minute[['지점', '일시', '기온(°C)', '일사(MJ/m^2)']]\n","  weather_5minute.columns = ['지점', '일시', '기온', '일사']\n","\n","  column_list = ['기온', '일사']\n","\n","  # 선형보간법 사용하여 결측값 채우기\n","  for idx, STN in enumerate(target_STN_list):\n","\n","      # ① 각 STN별로 데이터 나누기\n","      tmp = weather_5minute[weather_5minute['지점'] == STN]\n","\n","      # ② 시간순으로 정렬하기\n","      tmp = tmp.sort_values(by='일시')\n","\n","      # ③ 선형보간법을 사용하여 nan값 채우기                \n","      for column in column_list:\n","        tmp[column] = tmp[column].interpolate(method='linear')\n","\n","      # ④ tmp 합치기\n","      if idx == 0:\n","        interpolated_weather = tmp\n","      else:\n","        interpolated_weather = pd.concat([interpolated_weather, tmp])\n","\n","  # 전국 기상 대표값 정하기 (최고기온, 최저기온, 평균일사량)\n","  for idx, column in enumerate(column_list):\n","    if column == '기온':\n","      # 최고 기온\n","      tmp_1 = pd.DataFrame(interpolated_weather.groupby('일시', as_index = False)[column].max()).rename(columns = {'기온' : 'max_temp'})\n","      # 최저 기온\n","      tmp_2 = pd.DataFrame(interpolated_weather.groupby('일시', as_index = False)[column].min()).rename(columns = {'기온' : 'min_temp'})\n","      tmp = pd.merge(tmp_1, tmp_2, on = '일시', how = 'inner')\n","    else:\n","      # 평균 일사량\n","      tmp = pd.DataFrame(interpolated_weather.groupby('일시', as_index = False)[column].mean()).rename(columns = {'일사' : 'mean_insolation'})\n","    # ② groupby 결과를 인덱스 기준으로 Join\n","    if idx == 0:\n","      representative_weather = tmp\n","    else:\n","      representative_weather = pd.merge(representative_weather, tmp, on = '일시', how = 'inner')\n","\n","  representative_weather.rename(columns = {'일시' : 'datetime'}, inplace= True)\n","\n","  # 요일 특성 반영 (더미변수로 반영)\n","  # 0 : 월요일, 1 : 화요일 ~ 금요일, 2 : 토요일, 3 : 일요일\n","\n","  representative_weather['weekday'] = pd.to_datetime(representative_weather['datetime']).dt.weekday\n","  representative_weather.replace({'weekday' : {2 : 1, 3 : 1, 4 : 1, 5 : 2, 6 : 3}}, inplace = True)\n","  representative_weather = pd.get_dummies(representative_weather, columns = ['weekday'])\n","                                          \n","  # 휴일 유형에 맞춘 더미변수 반영\n","\n","  holiday = pd.read_csv(holiday_path)\n","\n","  for dateName in ['국회의원선거일', '대통령선거일', '동시지방선거일', '전국동시지방선거', '제21대 국회의원선거']:\n","    holiday.replace({'dateName' : {dateName : '선거일'}}, inplace = True)\n","  holiday.replace({'dateName' : {'1월1일' : '신정'}}, inplace = True)\n","  holiday.replace({'dateName' : {'부처님오신날' : '석가탄신일'}}, inplace = True)\n","  holiday.replace({'dateName' : {'어린이 날' : '어린이날'}}, inplace = True)\n","  holiday.replace({'dateName' : {'대체휴무일' : '대체공휴일'}}, inplace = True)\n","  holiday.replace({'dateName' : {'임시공휴일' : '대체공휴일'}}, inplace = True)\n","\n","  nameToNumber = {dateName : idx + 1  for idx, dateName in enumerate(list(holiday['dateName'].unique()))}\n","  holiday.replace({'dateName' : nameToNumber}, inplace = True)\n","  holiday_dict = {locdate : dateName for dateName, locdate in zip(holiday['dateName'], holiday['locdate'])}\n","\n","  for row_number in range(representative_weather.shape[0]):\n","    if representative_weather.at[row_number, 'datetime'][:-6] in list(holiday['locdate']):\n","      representative_weather.at[row_number, 'holiday'] = holiday_dict[representative_weather.at[row_number, 'datetime'][:-6]]\n","    else:\n","      representative_weather.at[row_number, 'holiday'] = 0\n","\n","  representative_weather = pd.get_dummies(representative_weather, columns = ['holiday'])\n","\n","  # 월, 시간 정보 반영 (더미변수로 반영)\n","\n","  representative_weather['month'] = pd.to_datetime(representative_weather['datetime']).dt.month\n","  representative_weather = pd.get_dummies(representative_weather, columns = ['month'])\n","  representative_weather['hour'] = pd.to_datetime(representative_weather['datetime']).dt.hour\n","  representative_weather = pd.get_dummies(representative_weather, columns = ['hour'])\n","\n","  return representative_weather"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":46550,"status":"ok","timestamp":1685482768744,"user":{"displayName":"누노누노","userId":"02523908322137540802"},"user_tz":-540},"id":"xZHiDZbMiWXp"},"outputs":[],"source":["# 기상대표값 데이터 생성하기\n","\n","weather_metadata_path = '/content/drive/MyDrive/Colab Notebooks/electricity_load_prediction/weather/weather_localcenter_metadata.csv'\n","weather_5minute_ASOS_path = '/content/drive/MyDrive/Colab Notebooks/electricity_load_prediction/weather/ASOS (2020-2023)/weather_5minute_ASOS_2020-2023.parquet'\n","holiday_path = '/content/drive/MyDrive/Colab Notebooks/electricity_load_prediction/time/holidays.csv'\n","representative_weather = weather_data_5minute(weather_metadata_path, weather_5minute_ASOS_path, holiday_path)"]},{"cell_type":"markdown","source":[],"metadata":{"id":"0h7pXsFkmNsO"}},{"cell_type":"markdown","metadata":{"id":"8O43ndpiGGbH"},"source":["전력 데이터는 지호님이 완성해주신 `power_demand_interpolated`를 사용하였습니다."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":1029,"status":"ok","timestamp":1685482769765,"user":{"displayName":"누노누노","userId":"02523908322137540802"},"user_tz":-540},"id":"UuSTnbOhBy77"},"outputs":[],"source":["# 전력 데이터 불러오기\n","\n","import pandas as pd\n","\n","power = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/electricity_load_prediction/load_supply/power_demand_interpolated.csv')\n","power.columns = ['datetime', 'load']"]},{"cell_type":"markdown","metadata":{"id":"s2aj7i7bpayD"},"source":["기상인자 대표값과 보간 처리된 전력데이터를 일시를 기준으로 Inner Join합니다."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":408,"status":"ok","timestamp":1685482770164,"user":{"displayName":"누노누노","userId":"02523908322137540802"},"user_tz":-540},"id":"0-x7VBwELdPq"},"outputs":[],"source":["# 전력 데이터와 기상인자 데이터 합치기\n","# 2023년 3월 19일까지의 데이터로 한정\n","\n","target_df = pd.merge(representative_weather, power, on = 'datetime', how = 'inner')[:-288]"]},{"cell_type":"markdown","metadata":{"id":"lKs5VX_HDF20"},"source":["Custom Dataset과 Dataloader를 사용하여 메모리 사용량을 줄였습니다."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":1536,"status":"ok","timestamp":1685482781244,"user":{"displayName":"누노누노","userId":"02523908322137540802"},"user_tz":-540},"id":"JL2uPxxUaUnt"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from sklearn.preprocessing import MinMaxScaler\n","import pandas as pd\n","import numpy as np\n","\n","class CustomDataset(torch.utils.data.Dataset):\n","  def __init__(self, target_df, binary_var_start_number, scaler_for_X, scaler_for_Y, seq_len, step_len, stride):\n","    self.dataset_tensor =torch.FloatTensor(np.array(pd.concat([pd.DataFrame(scaler_for_X.fit_transform(target_df.iloc[:,1:binary_var_start_number])),\n","                                                               target_df.iloc[:,binary_var_start_number:-1],\n","                                                               pd.DataFrame(scaler_for_Y.fit_transform(np.array(target_df['load']).reshape(-1, 1)))], axis = 1))).cuda()                           \n","    self.data_size = ((self.dataset_tensor.shape[0] - step_len - seq_len) // stride) + 1\n","    self.seq_len = seq_len\n","    self.step_len = step_len\n","    self.stride= stride\n","  \n","  def __len__(self):\n","    return self.data_size\n","  \n","  def __getitem__(self, idx):\n","    return self.dataset_tensor[idx*self.stride :idx*self.stride + self.seq_len, :], self.dataset_tensor[idx*self.stride + self.seq_len : idx*self.stride + self.seq_len + self.step_len, -1]\n","\n","# Scaler 객체 생성\n","ss_1 = MinMaxScaler() # Scaler for X\n","ss_2 = MinMaxScaler() # Scaler for y\n","\n","# CustomDataset 파라미터 설정 (Scaler 객체, 시퀀스의 길이, 스텝 사이즈, 건너뛸 간격)\n","seq_len = 2016 # 시퀀스의 길이\n","step_len = 72 # 스텝 사이즈 (추정하고자 하는 값의 개수)\n","stride = 1 # 건너뛸 간격\n","\n","# target_df를 train_df와 test_df로 분리\n","# target_df는 예측 대상 기간인 2023년 3월 13일부터 19일 + Sequence의 길이만큼의 데이터를 담음\n","train_df = target_df.iloc[:-8064,:]\n","valid_df = target_df.iloc[-8064:-2016,:]\n","test_df = target_df.iloc[-4032:,:]\n","\n","# 데이터셋 생성\n","train_dataset = CustomDataset(train_df, 4, ss_1, ss_2, seq_len, step_len, stride)\n","\n","# 데이터 로더 생성\n","train_dataloader = DataLoader(train_dataset, batch_size = 256, shuffle = True, drop_last = True)"]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","# CNN + 양방향 GRU 모델 구성하기\n","\n","in_channels = 58 # 입력 컬럼의 개수\n","out_channels = 116 # 합성곱 필터로 생성될 out의 Dimension = 필터의 개수\n","hidden_dim = 288 # 은닉 상태의 개수\n","output_dim = 72 # 출력 값의 개수(형태)\n","learning_rate = 0.0001 # 학습률\n","nb_epochs = 50 # 에포크의 수\n","seq_length = 2016 # sequence의 길이 (얼마간의 데이터가 들어오는가)\n","\n","class CNN_BIGRU(nn.Module):\n","  def __init__(self, in_channels, out_channels, kernel_size, hidden_dim, seq_len, output_dim, layers):\n","    super(CNN_BIGRU, self).__init__()\n","    # 입력인자 정의\n","    self.in_channels = in_channels\n","    self.out_channels = out_channels\n","    self.kernel_size = kernel_size\n","    self.hidden_dim = hidden_dim\n","    self.seq_len = seq_len\n","    self.output_dim = output_dim\n","    self.layers = layers\n","    # 합성곱 Layer 정의\n","    self.conv1d = nn.Conv1d(in_channels = self.in_channels, \n","                            out_channels = self.out_channels,\n","                            kernel_size = self.kernel_size, \n","                            padding = 'same')\n","    # GRU Layer 정의\n","    self.lstm = nn.GRU(self.out_channels, \n","                       self.hidden_dim, \n","                       num_layers = self.layers, \n","                       batch_first = True, \n","                       bidirectional = True)\n","    # 완전연결층 정의\n","    self.fc = nn.Linear(hidden_dim * 2, output_dim, bias = True)\n","\n","  def reset_hidden_state(self):\n","    self.hidden = (\n","        torch.zeros(self.layers, self.seq_len, self.hidden_dim),\n","        torch.zeros(self.layers, self.seq_len, self.hidden_dim)\n","        )\n","  \n","  # 결과값 계산\n","  def forward(self, x):\n","    x = torch.transpose(x, 1, 2)\n","    x = self.conv1d(x)\n","    x = torch.transpose(x, 1, 2)\n","    x, _status = self.lstm(x)\n","    x = self.fc(x[:, -1])\n","    return x"],"metadata":{"id":"FVDNDwW8PobB","executionInfo":{"status":"ok","timestamp":1685482786003,"user_tz":-540,"elapsed":329,"user":{"displayName":"누노누노","userId":"02523908322137540802"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5086,"status":"ok","timestamp":1685482796656,"user":{"displayName":"누노누노","userId":"02523908322137540802"},"user_tz":-540},"id":"j8ln1T5MlrS4","outputId":"19402169-c2d2-4188-817c-d075dc5ff6b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/519.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/519.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install torchmetrics -q"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":939,"status":"ok","timestamp":1685482802812,"user":{"displayName":"누노누노","userId":"02523908322137540802"},"user_tz":-540},"id":"rZeABdQyQCbq"},"outputs":[],"source":["# 모델 학습 함수 만들기\n","\n","# !pip install torchmetrics\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","from torchmetrics import MeanAbsolutePercentageError\n","\n","def train_model(model, train_df, num_epochs = None, lr = None, verbose = 10, patience = 10):\n","  optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n","  nb_epochs = num_epochs\n","  mean_abs_percentage_error = MeanAbsolutePercentageError().to(device)\n","\n","  # epoch마다 loss 저장\n","  train_hist = np.zeros(nb_epochs)\n","  output = []\n","  \n","  for epoch in range(nb_epochs):\n","    avg_cost = 0\n","    total_batch = len(train_df)\n","    for batch_idx, samples in enumerate(train_df):\n","      x_train, y_train = samples\n","      # seq별 hidden state reset\n","      model.reset_hidden_state()\n","      # H(x) 계산\n","      outputs = model(x_train)\n","      # cost 계산\n","      loss = mean_abs_percentage_error(outputs, y_train)\n","      # cost로 H(x) 개선\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      avg_cost += loss/total_batch \n","\n","    train_hist[epoch] = avg_cost\n","\n","    if epoch % verbose == 0:\n","      print('Epoch:', '%04d' % (epoch), 'train loss :', '{:.4f}'.format(avg_cost))\n","    # patience번째 마다 early stopping 여부 확인\n","    if (epoch % patience == 0) & (epoch != 0):\n","      if train_hist[epoch-patience] < train_hist[epoch]:\n","        print('\\n Early Stopping')\n","        break\n","  \n","  return model.eval(), train_hist"]},{"cell_type":"code","source":["# CNN + 양방향 GRU 모델 학습 \n","# 첫번째 학습\n","# 커널 사이즈 : 5, GRU layer의 개수 : 3\n","\n","kernel_size = 5 # 합성곱 필터의 kernel_size\n","\n","cnn_bigru = CNN_BIGRU(in_channels, out_channels, kernel_size, hidden_dim, seq_len, output_dim, layers = 3).to(device)\n","cnn_bigru_model_53, cnn_bigru_hist = train_model(cnn_bigru, train_dataloader, num_epochs = nb_epochs,\n","                                                 lr = learning_rate, verbose = 1, patience = 4)\n","torch.save(cnn_bigru_model_53, '/content/drive/MyDrive/Colab Notebooks/electricity_load_prediction/modeling/모델링 결과/model_20230531_CNN-BIGRU.pth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2zR-JsAHN0Mg","executionInfo":{"status":"ok","timestamp":1685466878060,"user_tz":-540,"elapsed":8400347,"user":{"displayName":"누노누노","userId":"02523908322137540802"}},"outputId":"85a7fd0d-09ad-4b04-8138-de152f4d46c6"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0000 train loss : 0.3741\n","Epoch: 0001 train loss : 0.3018\n","Epoch: 0002 train loss : 0.2772\n","Epoch: 0003 train loss : 0.2421\n","Epoch: 0004 train loss : 0.2202\n","Epoch: 0005 train loss : 0.2029\n","Epoch: 0006 train loss : 0.1781\n","Epoch: 0007 train loss : 0.1919\n","Epoch: 0008 train loss : 0.1682\n","Epoch: 0009 train loss : 0.1563\n","Epoch: 0010 train loss : 0.1426\n","Epoch: 0011 train loss : 0.1571\n","Epoch: 0012 train loss : 0.1394\n","Epoch: 0013 train loss : 0.1428\n","Epoch: 0014 train loss : 0.1282\n","Epoch: 0015 train loss : 0.1233\n","Epoch: 0016 train loss : 0.1255\n","Epoch: 0017 train loss : 0.1296\n","Epoch: 0018 train loss : 0.1198\n","Epoch: 0019 train loss : 0.1216\n","Epoch: 0020 train loss : 0.1150\n","Epoch: 0021 train loss : 0.1114\n","Epoch: 0022 train loss : 0.1198\n","Epoch: 0023 train loss : 0.1135\n","Epoch: 0024 train loss : 0.1050\n","Epoch: 0025 train loss : 0.1087\n","Epoch: 0026 train loss : 0.1063\n","Epoch: 0027 train loss : 0.1000\n","Epoch: 0028 train loss : 0.0920\n","Epoch: 0029 train loss : 0.0978\n","Epoch: 0030 train loss : 0.0938\n","Epoch: 0031 train loss : 0.0955\n","Epoch: 0032 train loss : 0.0945\n","\n"," Early Stopping\n"]}]},{"cell_type":"code","source":["cnn_bigru_model_53, cnn_bigru_hist = train_model(cnn_bigru_model_53.train(), train_dataloader, num_epochs = 10,\n","                                                lr = learning_rate, verbose = 1, patience = 4)\n","torch.save(cnn_bigru_model_53, '/content/drive/MyDrive/Colab Notebooks/electricity_load_prediction/modeling/모델링 결과/model_20230531_CNN-BIGRU.pth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-IpaqVqFFFw_","executionInfo":{"status":"ok","timestamp":1685487612803,"user_tz":-540,"elapsed":1360804,"user":{"displayName":"누노누노","userId":"02523908322137540802"}},"outputId":"900d4b44-a947-47b6-b2ac-03a85e21b137"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0000 train loss : 0.0861\n","Epoch: 0001 train loss : 0.0917\n","Epoch: 0002 train loss : 0.0904\n","Epoch: 0003 train loss : 0.0854\n","Epoch: 0004 train loss : 0.0836\n","Epoch: 0005 train loss : 0.0832\n","Epoch: 0006 train loss : 0.0852\n","Epoch: 0007 train loss : 0.0843\n","Epoch: 0008 train loss : 0.0775\n","Epoch: 0009 train loss : 0.0832\n"]}]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5878,"status":"ok","timestamp":1685488258094,"user":{"displayName":"누노누노","userId":"02523908322137540802"},"user_tz":-540},"id":"lkzjLRhIcROe","outputId":"44f080ff-5585-443a-c93a-fca4a89dd3b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["커널 사이즈가 5이고 GRU Layer가 3개인 CNN + Bi-GRU 모델의 평균 검증 MAPE는 1.736% 입니다\n"]}],"source":["# model validation (CNN + Bidirectional LSTM/GRU)\n","\n","# 커스텀데이터셋 클래스 정의하기\n","\n","import torch\n","\n","class CustomDatasetForTest(torch.utils.data.Dataset):\n","  def __init__(self, target_df, binary_var_start_number, scaler_for_X, scaler_for_Y, seq_len, step_len, stride):\n","    self.dataset_tensor =torch.FloatTensor(np.array(pd.concat([pd.DataFrame(scaler_for_X.transform(target_df.iloc[:,1:binary_var_start_number])),\n","                                                               target_df.iloc[:,binary_var_start_number:-1].reset_index(drop = True),\n","                                                               pd.DataFrame(scaler_for_Y.transform(np.array(target_df['load']).reshape(-1, 1)))], axis = 1))).cuda()                           \n","    self.data_size = ((self.dataset_tensor.shape[0] - step_len - seq_len) // stride)+1\n","    self.seq_len = seq_len\n","    self.step_len = step_len\n","    self.stride= stride\n","  \n","  def __len__(self):\n","    return self.data_size\n","  \n","  def __getitem__(self, idx):\n","    return self.dataset_tensor[idx*self.stride :idx*self.stride + self.seq_len, :], self.dataset_tensor[idx*self.stride + self.seq_len : idx*self.stride + self.seq_len + self.step_len, -1]\n","\n","# 데이터셋 및 데이터로더 객체 선언하기\n","\n","val_dataset = CustomDatasetForTest(valid_df, 4, ss_1, ss_2, seq_len, step_len, 72)\n","val_dataloader = DataLoader(val_dataset, batch_size = 1, shuffle = False, drop_last = True)\n","\n","with torch.no_grad():\n","  \n","  # 예측값과 실제값을 담을 빈 리스트 준비하기\n","\n","  cnnbigru_pred_list_53_val = []\n","  true_list = []\n","\n","  for X, y in val_dataloader:\n","\n","    # 데이터 로더에서 테스트 데이터 꺼내기\n","    X = X.to(device)\n","    y = y.to(device)\n","\n","    # 모델별 예측값 계산\n","    gru_pred_53 = cnn_bigru_model_53(X)\n","\n","    # 모델별 예측값 및 실제값을 리스트에 넣어주기\n","    cnnbigru_pred_list_53_val.append(ss_2.inverse_transform(gru_pred_53.cpu().detach().numpy()))\n","    true_list.append(ss_2.inverse_transform(y.cpu().detach().numpy()))\n","\n","# 위에서 완성된 예측값, 실제값 리스트로 평균 MAPE 계산하기\n","cnnbigru_MAPE_list_53_val = [np.mean(np.abs(pred - true)/true) for pred, true in zip(cnnbigru_pred_list_53_val, true_list)]\n","\n","# 모델별 평균 MAPE 출력하기\n","print('커널 사이즈가 5이고 GRU Layer가 3개인 CNN + Bi-GRU 모델의 평균 검증 MAPE는 {}% 입니다'.format(round(sum(cnnbigru_MAPE_list_53_val)/len(cnnbigru_MAPE_list_53_val) * 100, 3)))"]},{"cell_type":"code","source":["# model TEST (CNN + Bidirectional LSTM/GRU)\n","\n","# 커스텀데이터셋 클래스 정의하기\n","\n","class CustomDatasetForTest(torch.utils.data.Dataset):\n","  def __init__(self, target_df, binary_var_start_number, scaler_for_X, scaler_for_Y, seq_len, step_len, stride):\n","    self.dataset_tensor =torch.FloatTensor(np.array(pd.concat([pd.DataFrame(scaler_for_X.transform(target_df.iloc[:,1:binary_var_start_number])),\n","                                                               target_df.iloc[:,binary_var_start_number:-1].reset_index(drop = True),\n","                                                               pd.DataFrame(scaler_for_Y.transform(np.array(target_df['load']).reshape(-1, 1)))], axis = 1))).cuda()                           \n","    self.data_size = ((self.dataset_tensor.shape[0] - step_len - seq_len) // stride)+1\n","    self.seq_len = seq_len\n","    self.step_len = step_len\n","    self.stride= stride\n","  \n","  def __len__(self):\n","    return self.data_size\n","  \n","  def __getitem__(self, idx):\n","    return self.dataset_tensor[idx*self.stride :idx*self.stride + self.seq_len, :], self.dataset_tensor[idx*self.stride + self.seq_len : idx*self.stride + self.seq_len + self.step_len, -1]\n","\n","# 데이터셋 및 데이터로더 객체 선언하기\n","\n","test_dataset = CustomDatasetForTest(test_df, 4, ss_1, ss_2, seq_len, step_len, 72)\n","test_dataloader = DataLoader(test_dataset, batch_size = 1, shuffle = False, drop_last = True)\n","\n","with torch.no_grad():\n","  \n","  # 예측값과 실제값을 담을 빈 리스트 준비하기\n","\n","  cnnbigru_pred_list_53_test = []\n","  true_list = []\n","\n","  for X, y in test_dataloader:\n","\n","    # 데이터 로더에서 테스트 데이터 꺼내기\n","    X = X.to(device)\n","    y = y.to(device)\n","\n","    # 모델별 예측값 계산\n","    gru_pred_53 = cnn_bigru_model_53(X)\n","\n","    # 모델별 예측값 및 실제값을 리스트에 넣어주기\n","    cnnbigru_pred_list_53_test.append(ss_2.inverse_transform(gru_pred_53.cpu().detach().numpy()))\n","    true_list.append(ss_2.inverse_transform(y.cpu().detach().numpy()))\n","\n","# 위에서 완성된 예측값, 실제값 리스트로 평균 MAPE 계산하기\n","cnnbigru_MAPE_list_53_test = [np.mean(np.abs(pred - true)/true) for pred, true in zip(cnnbigru_pred_list_53_test, true_list)]\n","\n","# 모델별 평균 MAPE 출력하기\n","print('커널 사이즈가 5이고 GRU Layer가 3개인 CNN + Bi-GRU 모델의 평균 테스트 MAPE는 {}% 입니다'.format(round(sum(cnnbigru_MAPE_list_53_test)/len(cnnbigru_MAPE_list_53_test) * 100, 3)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gj8lxolHPHqU","executionInfo":{"status":"ok","timestamp":1685488269707,"user_tz":-540,"elapsed":3129,"user":{"displayName":"누노누노","userId":"02523908322137540802"}},"outputId":"a38abf82-10de-405b-b6f0-09a817239473"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["커널 사이즈가 5이고 GRU Layer가 3개인 CNN + Bi-GRU 모델의 평균 테스트 MAPE는 2.049% 입니다\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1ccjPOx3afKu3TJGRjqfv8l4HKs4TTcRf","timestamp":1684965355183}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}